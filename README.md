# Improving the Perturbation-Based Explanation of Deepfake Detectors Through the Use of Adversarially-Generated Samples

## PyTorch Implementation [[Paper](https://TBA)] [[DOI](https://TBA)] [[Cite](#citation)]
- From **"Improving the Perturbation-Based Explanation of Deepfake Detectors Through the Use of Adversarially-Generated Samples"**, Proc. IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW 2025) <br />
- Written by Konstantinos Tsigos, Evlampios Apostolidis and Vasileios Mezaris. <br />
- This software can be used to evaluate and compare the performance of four perturbation-based explanation methods from the literature (LIME, SHAP, SOBOL, RISE) on explaining the output of a state-of-the-art model for deepfake detection, with the performance of modified versions of them, that use adversarially-generated samples of the input image to form perturbation masks and infer the importance of different input features. The employed evaluation framework assesses the performance of an explanation method by examining the extent to which the image regions that were found as the most important ones, can be used to flip the deepfake detector's decision through a series of adversarial attacks.

## Dependencies
The code was developed, checked and verified on an `Ubuntu 20.04.6` PC with an `NVIDIA RTX 4090` GPU and an `i5-12600K` CPU. All dependencies can be found inside the [environment.yml](/environment.yml) file, which can be used to set up the necessary [conda](https://docs.conda.io/en/latest/) enviroment.

## Data
<div align="justify">

The data for re-producing our experiments are the videos from the test split of the [FaceForensics++](https://github.com/ondyari/FaceForensics) dataset. This dataset contains 1000 original videos and 4000 fake videos created using one of the following four classes of AI-based manipulation (1000 per class): "FaceSwap" (FS), "DeepFakes" (DF), "Face2Face" (F2F),  and "NeuralTextures" (NT). The videos of the FS class were created via a graphics-based approach that transfers the face region from a source to a target video. The videos of the DF class were produced using autoencoders to replace a face in a target sequence with a face in a source video or image collection. The videos of the F2F class were obtained by a facial reenactment system that transfers the expressions of a source to a target video while maintaining the identity of the target person. The videos of the NT class were generated by modifying the facial expressions corresponding to the mouth region, using a patch-based GAN-loss. The dataset is divided into training, validation, and test sets, comprised of 720, 140 and 140 videos, respectively.

To re-create the database file that we used in our experiments, please follow the steps below:

1. Download the entire [FaceForensics++](https://github.com/ondyari/FaceForensics#Access) dataset
2. Run the following script to preprocess the raw data:
```bash
python data/preprocess_ff.py prepro -r RAW_DATA_PATH -tr PREPROCESSED_DATA_PATH -d cuda:0 -mdcsv RAW_DATA_PATH/dataset_info.csv -orig
```
where `RAW_DATA_PATH` is the path to the downloaded FF++ dataset and `PREPROCESSED_DATA_PATH` is the path to save the preprocessed data. The script will create a new file `faceforensics_frames.csv` containing the paths to the preprocessed frames.

3. Create a new LMDB database by running the following script:
```bash
python data/lmdb_storage.py add-csv -csv faceforensics_frames.csv -h -pc relative_path -d ./data/xai_test_data.lmdb -ms 21474836480 -v -b PREPROCESSED_DATA_PATH
```
where `faceforensics_frames.csv` is the file created in the previous step and `PREPROCESSED_DATA_PATH` is the path to the preprocessed data. The script will create a new LMDB database `xai_test_data.lmdb` containing the preprocessed frames. The `-ms` flag specifies the maximum size of the database in bytes, default is 20GB.
</div>

## Trained model
<div align="justify">

The employed model (called ff_attribution) was trained for multiclass classification on the FaceForensics++ dataset. It outputs a probability for each of the 5 classes (0, 1, 2, 3, 4), corresponding to "Real", "NeuralTextures", "Face2Face", "DeepFakes" and "FaceSwap", respectively.

#### Model characteristics
| Model | ff_attribution
| --- | --- |
| Task | multiclass |
| Arch. | efficientnetv2_b0 |
| Type | CNN |
| No. Params | 7.1M |
| No. Datasets | 1 |
| Input | (B, 3, 224, 224) |
| Output | (B, 5) |

#### Performance (FF++ test set)
| Metric | Value |
| --- | --- |
| MulticlassAccuracy | 0.9626 |
| MulticlassAUROC | 0.9970 |
| MulticlassF1Score | 0.9627 |
| MulticlassAveragePrecision | 0.9881 |

## Evaluation and visualization
<div align="justify">

To evaluate the explanation method(s) using our framework, run the [`evaluate.py`](explanation/evaluate.py) script.

To visualize the created explanation by an explanation method, for a specific image of the dataset, run the [`visualize.py`](explanation/visualize.py) script.

To compare the runtime and the number of inferences of our modified explanation methods with their original implementations, run the [`efficiency.py`](explanation/efficiency.py) script.

## Running parameters and evaluation results
<div align="justify">
The default parameters and the available options for running the above scripts, are listed below: 

|Parameter name | File | Description | Default Value | Options
| :--- | :--- | :--- | :---: | :---:
`evaluation_explanation_method`|[`evaluate.py`](explanation/evaluate.py#L18:L19) | Explanation method(s) to evaluate | 'All' | 'All', 'LIME', 'LIME_adv', 'SHAP', 'SHAP_adv', 'SOBOL', 'SOBOL_adv', 'RISE', 'RISE_adv'
`explanation_method`|[`visualize.py`](explanation/visualize.py#L20:L21)| Explanation method to explain the image | 'LIME' | 'LIME', 'SHAP', 'SOBOL', 'RISE'
`adversarial_mask`|[`visualize.py`](explanation/visualize.py#L22:L23)| Use the adversarial counterpart of the selected image as a mask to create the pertubations of the explanation method | 'Both' | 'Yes', 'No', 'Both'
`dataset_example_index`|[`visualize.py`](explanation/visualize.py#L24:L25)| Index of the image in the database | 'random' | 'random', integer between [0,13837]

The evaluation results are printed onto the console and saved into a CSV file which is stored within the `results` folder, created at the [explanation](/explanation) path. To eliminate the need to run the evaluation process from the beginning in case of an unexpected error, the evaluation results for each test image are accumulated into an NPY file, that is used as a checkpoint and is also located at the `results` folder.

## Citation
<div align="justify">
    
If you find our work, code or pretrained models, useful in your work, please cite the following publication:

K. Tsigos, E. Apostolidis, V. Mezaris, "<b>Improving the Perturbation-Based Explanation of Deepfake Detectors Through the Use of Adversarially-Generated Samples</b>", Proc. IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW 2025), Feb.-Mar. 2025, Tucson, Arizona, US.
</div>

BibTeX:

```
@INPROCEEDINGS{AI4MFDD_2025,
  author={Tsigos, Konstantinos and Apostolidis, Evlampios and Mezaris, Vasileios},
  booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)}, 
  title={Improving the Perturbation-Based Explanation of Deepfake Detectors Through the Use of Adversarially-Generated Samples}, 
  year={2025},
  volume={},
  number={},
  pages={x-x},
  doi={xxx}
}
```

## License
<div align="justify">
    
This code is provided for academic, non-commercial use only. Please also check for any restrictions applied in the code parts and datasets used here from other sources. For the materials not covered by any such restrictions, redistribution and use in source and binary forms, with or without modification, are permitted for academic non-commercial use provided that the following conditions are met:

Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation provided with the distribution.

This software is provided by the authors "as is" and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. In no event shall the authors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software, even if advised of the possibility of such damage.
</div>

## Acknowledgement
<div align="justify"> This work was supported by the EU Horizon Europe program under grant agreement 101070190 AI4TRUST. </div>
